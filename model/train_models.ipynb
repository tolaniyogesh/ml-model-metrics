{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Classification Models\n",
    "## Breast Cancer Wisconsin Dataset\n",
    "\n",
    "This notebook trains 6 classification models and evaluates them using multiple metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, roc_auc_score, precision_score, \n",
    "    recall_score, f1_score, matthews_corrcoef,\n",
    "    confusion_matrix, classification_report\n",
    ")\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_breast_cancer()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = pd.Series(data.target, name='target')\n",
    "\n",
    "print(f\"Dataset Shape: {X.shape}\")\n",
    "print(f\"Number of Features: {X.shape[1]}\")\n",
    "print(f\"Number of Instances: {X.shape[0]}\")\n",
    "print(f\"\\nTarget Distribution:\")\n",
    "print(y.value_counts())\n",
    "print(f\"\\nFeature Names:\")\n",
    "print(X.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train-Test Split (80-20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = X_train.copy()\n",
    "train_data['target'] = y_train.values\n",
    "train_data.to_csv('../train_data.csv', index=False)\n",
    "\n",
    "test_data = X_test.copy()\n",
    "test_data['target'] = y_test.values\n",
    "test_data.to_csv('../test_data.csv', index=False)\n",
    "\n",
    "X_test.to_csv('../test_data_without_labels.csv', index=False)\n",
    "\n",
    "print(\"Saved CSV files:\")\n",
    "print(\"- train_data.csv (with target labels)\")\n",
    "print(\"- test_data.csv (with target labels)\")\n",
    "print(\"- test_data_without_labels.csv (for predictions only)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "with open('scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "print(\"Scaler saved to scaler.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model_name, y_true, y_pred, y_pred_proba=None):\n",
    "    metrics = {\n",
    "        'Model': model_name,\n",
    "        'Accuracy': accuracy_score(y_true, y_pred),\n",
    "        'Precision': precision_score(y_true, y_pred, average='binary', zero_division=0),\n",
    "        'Recall': recall_score(y_true, y_pred, average='binary', zero_division=0),\n",
    "        'F1': f1_score(y_true, y_pred, average='binary', zero_division=0),\n",
    "        'MCC': matthews_corrcoef(y_true, y_pred)\n",
    "    }\n",
    "    \n",
    "    if y_pred_proba is not None:\n",
    "        try:\n",
    "            metrics['AUC'] = roc_auc_score(y_true, y_pred_proba)\n",
    "        except:\n",
    "            metrics['AUC'] = 0.0\n",
    "    else:\n",
    "        metrics['AUC'] = 0.0\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train Models and Evaluate\n",
    "\n",
    "### 5.1 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = LogisticRegression(max_iter=10000, random_state=42)\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "lr_pred = lr_model.predict(X_test_scaled)\n",
    "lr_pred_proba = lr_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "lr_metrics = evaluate_model('Logistic Regression', y_test, lr_pred, lr_pred_proba)\n",
    "print(\"Logistic Regression Metrics:\")\n",
    "for key, value in lr_metrics.items():\n",
    "    if key != 'Model':\n",
    "        print(f\"{key}: {value:.4f}\")\n",
    "\n",
    "with open('model_logistic_regression.pkl', 'wb') as f:\n",
    "    pickle.dump(lr_model, f)\n",
    "print(\"\\nModel saved to model_logistic_regression.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_model = DecisionTreeClassifier(random_state=42, max_depth=10)\n",
    "dt_model.fit(X_train, y_train)\n",
    "dt_pred = dt_model.predict(X_test)\n",
    "dt_pred_proba = dt_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "dt_metrics = evaluate_model('Decision Tree', y_test, dt_pred, dt_pred_proba)\n",
    "print(\"Decision Tree Metrics:\")\n",
    "for key, value in dt_metrics.items():\n",
    "    if key != 'Model':\n",
    "        print(f\"{key}: {value:.4f}\")\n",
    "\n",
    "with open('model_decision_tree.pkl', 'wb') as f:\n",
    "    pickle.dump(dt_model, f)\n",
    "print(\"\\nModel saved to model_decision_tree.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 K-Nearest Neighbor Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_model.fit(X_train_scaled, y_train)\n",
    "knn_pred = knn_model.predict(X_test_scaled)\n",
    "knn_pred_proba = knn_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "knn_metrics = evaluate_model('K-Nearest Neighbor', y_test, knn_pred, knn_pred_proba)\n",
    "print(\"K-Nearest Neighbor Metrics:\")\n",
    "for key, value in knn_metrics.items():\n",
    "    if key != 'Model':\n",
    "        print(f\"{key}: {value:.4f}\")\n",
    "\n",
    "with open('model_k-nearest_neighbor.pkl', 'wb') as f:\n",
    "    pickle.dump(knn_model, f)\n",
    "print(\"\\nModel saved to model_k-nearest_neighbor.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Naive Bayes Classifier (Gaussian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_model = GaussianNB()\n",
    "nb_model.fit(X_train_scaled, y_train)\n",
    "nb_pred = nb_model.predict(X_test_scaled)\n",
    "nb_pred_proba = nb_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "nb_metrics = evaluate_model('Naive Bayes', y_test, nb_pred, nb_pred_proba)\n",
    "print(\"Naive Bayes Metrics:\")\n",
    "for key, value in nb_metrics.items():\n",
    "    if key != 'Model':\n",
    "        print(f\"{key}: {value:.4f}\")\n",
    "\n",
    "with open('model_naive_bayes.pkl', 'wb') as f:\n",
    "    pickle.dump(nb_model, f)\n",
    "print(\"\\nModel saved to model_naive_bayes.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Random Forest (Ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "rf_pred = rf_model.predict(X_test)\n",
    "rf_pred_proba = rf_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "rf_metrics = evaluate_model('Random Forest', y_test, rf_pred, rf_pred_proba)\n",
    "print(\"Random Forest Metrics:\")\n",
    "for key, value in rf_metrics.items():\n",
    "    if key != 'Model':\n",
    "        print(f\"{key}: {value:.4f}\")\n",
    "\n",
    "with open('model_random_forest.pkl', 'wb') as f:\n",
    "    pickle.dump(rf_model, f)\n",
    "print(\"\\nModel saved to model_random_forest.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 XGBoost (Ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = XGBClassifier(n_estimators=100, random_state=42, eval_metric='logloss')\n",
    "xgb_model.fit(X_train, y_train)\n",
    "xgb_pred = xgb_model.predict(X_test)\n",
    "xgb_pred_proba = xgb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "xgb_metrics = evaluate_model('XGBoost', y_test, xgb_pred, xgb_pred_proba)\n",
    "print(\"XGBoost Metrics:\")\n",
    "for key, value in xgb_metrics.items():\n",
    "    if key != 'Model':\n",
    "        print(f\"{key}: {value:.4f}\")\n",
    "\n",
    "with open('model_xgboost.pkl', 'wb') as f:\n",
    "    pickle.dump(xgb_model, f)\n",
    "print(\"\\nModel saved to model_xgboost.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comparison of All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics = [lr_metrics, dt_metrics, knn_metrics, nb_metrics, rf_metrics, xgb_metrics]\n",
    "results_df = pd.DataFrame(all_metrics)\n",
    "results_df = results_df[['Model', 'Accuracy', 'AUC', 'Precision', 'Recall', 'F1', 'MCC']]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL RESULTS COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "results_df.to_csv('../model_results.csv', index=False)\n",
    "print(\"\\nResults saved to model_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
